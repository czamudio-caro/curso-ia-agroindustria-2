\documentclass[11pt, a4paper]{article}

% --- MOTOR DE FUENTES (XeLaTeX) ---
\usepackage{fontspec}
\setmainfont{DejaVu Sans}[
    BoldFont={DejaVu Sans Bold},
    ItalicFont={DejaVu Sans Oblique},
    Scale=0.9
]
\setmonofont{DejaVu Sans Mono}[Scale=0.8]

% --- IDIOMA ---
\usepackage{polyglossia}
\setmainlanguage{spanish}

% --- PAQUETES ---
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{listings}
\usepackage[most]{tcolorbox}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{colortbl}
\usepackage{caption}
\usepackage{subcaption}
\usetikzlibrary{shapes, arrows, positioning, babel, matrix, backgrounds, shadows}

% --- GEOMETR√çA ---
\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}
\setlength{\headheight}{28pt}
\setlength{\parskip}{0.5em}

% --- COLORES ---
\definecolor{primary}{RGB}{0, 85, 164}        % Azul Ingenier√≠a
\definecolor{accent}{RGB}{34, 139, 34}        % Verde Agro
\definecolor{danger}{RGB}{204, 0, 0}          % Rojo Alerta
\definecolor{pandas}{RGB}{19, 7, 84}          % Azul oscuro (Pandas)
\definecolor{codebg}{RGB}{245, 247, 250}
\definecolor{warning}{RGB}{255, 165, 0}       % Naranja
\definecolor{industry}{RGB}{70, 130, 180}     % Azul industrial

% --- CAJAS PERSONALIZADAS ---
\newtcolorbox{conceptbox}[1]{
    colback=blue!5!white,
    colframe=primary,
    title=#1,
    fonttitle=\bfseries,
    boxrule=0.8mm,
    arc=2mm,
    shadow={2mm}{-2mm}{0mm}{black!20}
}

\newtcolorbox{agrobox}[1]{
    colback=green!5!white,
    colframe=accent,
    title=\textbf{üå±} #1,
    fonttitle=\bfseries,
    boxrule=0.8mm,
    arc=2mm
}

\newtcolorbox{warningbox}[1]{
    colback=red!5!white,
    colframe=danger,
    title=\textbf{‚ö†} #1,
    fonttitle=\bfseries,
    boxrule=0.8mm,
    arc=2mm
}

\newtcolorbox{ethicsbox}[1]{
    colback=yellow!5!white,
    colframe=orange!75!black,
    title=\textbf{‚öñ} #1,
    fonttitle=\bfseries,
    boxrule=0.8mm,
    arc=2mm
}

\newtcolorbox{industrybox}[1]{
    colback=cyan!5!white,
    colframe=industry,
    title=\textbf{üè≠} #1,
    fonttitle=\bfseries,
    boxrule=0.8mm,
    arc=2mm
}

\newtcolorbox{sciencebox}[1]{
    colback=violet!5!white,
    colframe=violet!75!black,
    title=\textbf{üî¨} #1,
    fonttitle=\bfseries,
    boxrule=0.8mm,
    arc=2mm
}

% --- ESTILO DE C√ìDIGO ---
\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{codebg},
    commentstyle=\color{gray}\itshape,
    keywordstyle=\color{pandas}\bfseries,
    numberstyle=\tiny\color{gray},
    stringstyle=\color{accent},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=l,
    rulecolor=\color{pandas},
    numbers=left,
    showstringspaces=false,
    literate=
        {√°}{{\'a}}1 {√©}{{\'e}}1 {√≠}{{\'i}}1 {√≥}{{\'o}}1 {√∫}{{\'u}}1 {√±}{{\~n}}1
        {‚ö†}{{\textcolor{orange}{\bfseries !}}}1
        {NaN}{{\textcolor{red}{\bfseries NaN}}}3
        {None}{{\textcolor{red}{\bfseries None}}}4
}

\lstset{style=pythonstyle}

% --- ENCABEZADO ---
\pagestyle{fancy}
\fancyhf{}
\lhead{\textbf{Ingenier√≠a de IA I}}
\rhead{Semana 03: Pandas para Agroindustria}
\rfoot{P√°gina \thepage}

\title{\textbf{Pandas para Procesamiento Industrial}\\
       \large Trazabilidad, Control de Calidad y An√°lisis de Producci√≥n\\
       Agroindustria Alimenticia 4.0}
\author{Curso de IA Aplicada al Agro\\
        Universidad del Valle}
\date{Enero 2026}

\begin{document}

\maketitle

\begin{abstract}
Este manual introduce Pandas desde la perspectiva de la ingenier√≠a de datos aplicada a la industria alimenticia. A diferencia del enfoque tradicional basado en an√°lisis exploratorio gen√©rico, aqu√≠ abordamos problemas reales de trazabilidad de lotes, control estad√≠stico de procesos (SPC), cumplimiento normativo (HACCP, FDA) y optimizaci√≥n de l√≠neas de producci√≥n. Los estudiantes aprender√°n a procesar datasets heterog√©neos (fechas, categor√≠as, mediciones num√©ricas) con eficiencia computacional y rigor cient√≠fico.
\end{abstract}

\tableofcontents
\newpage
\section*{Prefacio: Del Campo a la Mesa (y al Cloud)}

En la Semana 02 trabajaste con NumPy procesando matrices num√©ricas homog√©neas (humedad del suelo en 365 d√≠as √ó 100 zonas). Ese enfoque es excelente para c√°lculo matricial puro, pero \textbf{la agroindustria 4.0 genera datos heterog√©neos y desordenados}.

Un ingeniero de datos en la industria real se enfrenta a tres desaf√≠os que NumPy no resuelve por s√≠ solo:

\begin{itemize}
    \item \textbf{Heterogeneidad de Tipos (Mixed Data Types)}: A diferencia de una matriz matem√°tica, un registro industrial mezcla texto, n√∫meros y tiempo.
    \\ \textit{Ejemplo:} Un lote de caf√© tiene ID (\texttt{string}), timestamp de entrada (\texttt{datetime}), temperatura (\texttt{float}), operario (\texttt{category}) y aprobaci√≥n de calidad (\texttt{bool}).

    \item \textbf{Datos Sucios y Series Irregulares (Handling Missing Data)}: En el mundo real, los sensores fallan. Si un term√≥metro IoT env√≠a datos cada 30 segundos pero se apaga por un corte de luz, tendr√°s "huecos" temporales.
    \\ \textit{Reto Pro:} No puedes simplemente borrar esos huecos; debes decidir si imputar el valor (rellenarlo estad√≠sticamente) o alertar al sistema.

    \item \textbf{Relaciones Relacionales (Merging \& Joins)}: La informaci√≥n nunca est√° en una sola tabla. Para gestionar un \textit{Recall} (retiro de producto por seguridad), debes cruzar trazabilidad compleja: \texttt{lotes\_producidos} $\leftrightarrow$ \texttt{pruebas\_laboratorio} $\leftrightarrow$ \texttt{despachos\_clientes}.
\end{itemize}

NumPy no est√° dise√±ado para bases de datos relacionales. \textbf{Pandas s√≠}. Es la herramienta est√°ndar para construir lo que en la industria llamamos \textit{ETL (Extract, Transform, Load)}.

\begin{industrybox}{Contexto Industrial: Por qu√© Excel no es suficiente}
Imagina una planta procesadora de alimentos que opera 24/7 en 3 turnos. Aunque el volumen de datos parezca manejable al inicio, la complejidad escala exponencialmente:

\begin{itemize}
    \item \textbf{Volumen:} 14,400 lotes/a√±o √ó 20 variables = 288,000 puntos de datos anuales.
    \item \textbf{Historico:} En 5 a√±os (requerido por ley), son casi \textbf{1.5 millones de registros}.
    \item \textbf{El Riesgo:} En Excel, un error de "copiar y pegar" o una f√≥rmula arrastrada mal puede costar una certificaci√≥n de calidad.
\end{itemize}

\textbf{La diferencia clave:} Un script de Pandas es \textbf{auditable} y \textbf{reproducible}. Si un auditor de la FDA (Food and Drug Administration) pide explicar un c√°lculo, puedes mostrar el c√≥digo. En una hoja de c√°lculo manual, eso es imposible.
\end{industrybox}

\begin{sciencebox}{Concepto Avanzado: El Gemelo Digital (Digital Twin)}
En este curso no solo "analizaremos tablas". Vamos a construir una representaci√≥n digital de la planta.
Nuestro DataFrame de Pandas actuar√° como un \textbf{Digital Twin} de bajo nivel: un espejo matem√°tico que refleja el estado exacto de la producci√≥n, permiti√©ndonos detectar anomal√≠as (ej. fermentaci√≥n excesiva) antes de que el producto f√≠sico se da√±e.
\end{sciencebox}

\newpage

\section{Cap√≠tulo I: Fundamentos ‚Äî DataFrame como Base de Datos en Memoria}

\subsection{La Anatom√≠a de un DataFrame}

Un DataFrame es una \textbf{tabla en memoria RAM} con √≠ndice expl√≠cito y columnas etiquetadas, similar a una tabla SQL, pero optimizada para an√°lisis en Python.[web:18][web:19] En ingenier√≠a de datos lo usamos como ``mini base de datos en memoria'' para aplicar transformaciones, validar la calidad y luego escribir resultados hacia sistemas m√°s grandes (SQL, data lakes, etc.).[web:20][web:21][web:29]

A diferencia de NumPy (donde accedes por posici√≥n), Pandas permite consultas tipo SQL.

\begin{center}
\begin{tikzpicture}[
    node distance=1.5cm,
    box/.style={rectangle, draw, minimum width=3cm, minimum height=1.2cm, align=center, rounded corners},
    arrow/.style={->, thick, >=stealth}
]
    % Componentes
    \node[box, fill=blue!10] (index) {\textbf{Index}\\ (Etiquetas de filas)};
    \node[box, fill=green!10, right=of index] (columns) {\textbf{Columns}\\ (Nombres de variables)};
    \node[box, fill=orange!10, below=of index] (values) {\textbf{Values}\\ (NumPy array 2D)};
    \node[box, fill=violet!10, below=of columns] (dtypes) {\textbf{Dtypes}\\ (Tipos por columna)};

    % Relaciones
    \draw[arrow] (index) -- (values) node[midway, left, font=\tiny] {Mapeo de filas};
    \draw[arrow] (columns) -- (values) node[midway, right, font=\tiny] {Mapeo de columnas};
    \draw[arrow] (columns) -- (dtypes) node[midway, right, font=\tiny] {Especifica tipos};
\end{tikzpicture}
\end{center}

La parte m√°s subestimada por principiantes son los \textbf{dtypes}: definen si una columna se comporta como n√∫mero, fecha, texto o categor√≠a.[web:18][web:19] Elegir bien los tipos de datos es el primer paso hacia un c√≥digo m√°s r√°pido, con menos errores y m√°s cercano a los est√°ndares de ingenier√≠a profesional.[web:23][web:24]

\textbf{Diferencia clave con NumPy}:
\begin{itemize}
    \item NumPy: \texttt{array[0, 3]} ‚Üí Posici√≥n absoluta (fila 0, columna 3)
    \item Pandas: \texttt{df.loc["2026-01-01", "Temperatura"]} ‚Üí Etiqueta sem√°ntica
\end{itemize}

\subsection{Series vs DataFrame}

\begin{table}[h]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Caracter√≠stica} & \textbf{Series} & \textbf{DataFrame} \\
\midrule
Dimensionalidad & 1D (columna √∫nica) & 2D (tabla) \\
Tipo de datos & Homog√©neo (un dtype) & Heterog√©neo (dtype por columna) \\
Index & S√≠ & S√≠ \\
Operaciones & Vectorizadas & Por columna/fila \\
Uso t√≠pico & Una medici√≥n & Dataset completo \\
\bottomrule
\end{tabular}
\caption{Comparaci√≥n Series-DataFrame}
\end{table}

En proyectos peque√±os puedes trabajar solo con DataFrames, pero en proyectos industriales es com√∫n combinar \textbf{Series} para c√°lculos r√°pidos (estad√≠sticos o de control) y \textbf{DataFrames} como capa principal de integraci√≥n de datos de m√∫ltiples sistemas (SCADA, LIMS, ERP).[web:25][web:31]

\begin{lstlisting}[language=Python, caption={Crear Series y DataFrame desde c√≥digo}]
import pandas as pd
import numpy as np

# Series: Una columna de temperaturas
temps = pd.Series([72.5, 73.1, 72.8, 74.0],
                  index=['Lote_A', 'Lote_B', 'Lote_C', 'Lote_D'],
                  name='Temperatura_Pasteurizacion')

print(temps['Lote_B'])  # Acceso por etiqueta ‚Üí 73.1

# DataFrame: Tabla completa de un turno
data = {
    'id_lote': ['L001', 'L002', 'L003'],
    'temp_C': [72.5, 73.1, 71.9],
    'presion_bar': [2.8, 2.9, 2.7],
    'resultado_QA': ['Aprobado', 'Aprobado', 'Rechazado']
}

df = pd.DataFrame(data)
print(df.dtypes)
\end{lstlisting}

\subsection{Carga de Datos Industriales}

Desde la perspectiva de ingenier√≠a de datos, este paso se conoce como \textbf{ingesti√≥n de datos}: recibir informaci√≥n cruda desde sistemas operativos (OT) y sistemas de negocio (IT) y convertirla en DataFrames consistentes para an√°lisis posterior.[web:25][web:28]

En la industria, los datos vienen de m√∫ltiples fuentes:

\begin{itemize}
    \item \textbf{SCADA} (sistemas de control): CSV/Excel exportados desde sistemas que hablan protocolos industriales (OPC UA, Modbus, MQTT), casi siempre con timestamps.[web:25][web:28]
    \item \textbf{LIMS} (laboratorio): Resultados en archivos Excel con hojas m√∫ltiples
    \item \textbf{ERP} (SAP/Oracle): Exportaciones CSV con separadores raros y codificaciones regionales
    \item \textbf{Sensores IoT}: JSON desde APIs REST
\end{itemize}

\begin{lstlisting}[language=Python, caption={Carga robusta de datos industriales}]
import pandas as pd
import requests

# 1. CSV con problemas comunes (SCADA)
df_scada = pd.read_csv(
    'datos_scada.csv',
    sep=';',                      # Separador europeo
    decimal=',',                  # Decimales con coma
    encoding='latin1',            # Codificaci√≥n Windows
    parse_dates=['timestamp'],    # Convertir a datetime autom√°ticamente
    na_values=['error', 'offline', '-'],  # Valores nulos personalizados
    dtype={'id_lote': str}        # Forzar ID como texto (evita 001 ‚Üí 1)
)

# 2. Excel con m√∫ltiples hojas (LIMS)
df_lab = pd.read_excel(
    'resultados_laboratorio.xlsx',
    sheet_name='Microbiologia',   # Hoja espec√≠fica
    header=2,                     # La fila 3 tiene los t√≠tulos
    usecols='A:F'                 # Solo columnas A-F
)

# 3. JSON desde API de sensor IoT
response = requests.get('https://api.sensores.com/temperatura')
df_temp = pd.DataFrame(response.json()['data'])

# Buen h√°bito de ingeniero de datos:
# despu√©s de cargar, siempre revisa esquema y tipos
print(df_scada.info())     # Ver columnas, tipos y nulos
print(df_lab.head())       # Ver las primeras filas
print(df_temp.dtypes)      # Ver tipos inferidos en JSON
\end{lstlisting}

\newpage

\section{Cap√≠tulo II: Indexaci√≥n y Selecci√≥n ‚Äî El Fundamento de Todo}

\subsection{Los 3 M√©todos de Acceso}

\begin{warningbox}{Error \#1 m√°s com√∫n en Pandas}
Confundir \texttt{.loc[]} (etiquetas) con \texttt{.iloc[]} (posiciones). Esto causa bugs silenciosos cuando el √≠ndice no es secuencial.
\end{warningbox}

\begin{table}[h]
\centering
\rowcolors{2}{gray!10}{white}
\begin{tabular}{@{}lp{5cm}p{6cm}@{}}
\toprule
\textbf{M√©todo} & \textbf{Qu√© usa} & \textbf{Ejemplo industrial} \\
\midrule
\texttt{.loc[]} & Etiquetas (labels) & \texttt{df.loc["2026-01-15", "pH"]} \\
\texttt{.iloc[]} & Posici√≥n (enteros) & \texttt{df.iloc[0, 3]} (primera fila, cuarta columna) \\
\texttt{df[]} & Columnas (principalmente) & \texttt{df["Temperatura"]} \\
\bottomrule
\end{tabular}
\caption{M√©todos de indexaci√≥n en Pandas}
\end{table}

En ingenier√≠a de datos industrial, el 80\% de los errores de producci√≥n provienen de indexaci√≥n incorrecta.[web:23] Elegir el m√©todo correcto no es solo sintaxis ‚Äî es una decisi√≥n de \textbf{robustez} y \textbf{mantenibilidad}.[web:26][web:29]

\begin{lstlisting}[language=Python, caption={Ejemplos de indexaci√≥n}]
import pandas as pd

# Dataset simulado: Control de calidad de leche
data = {
    'id_lote': ['L001', 'L002', 'L003', 'L004'],
    'fecha': ['2026-01-10', '2026-01-10', '2026-01-11', '2026-01-11'],
    'temp_pasteurizacion': [72.5, 73.0, 71.8, 74.2],
    'ph': [6.7, 6.6, 6.5, 6.9],
    'resultado': ['Aprobado', 'Aprobado', 'Rechazado', 'Aprobado']
}

df = pd.DataFrame(data)

# 1. SELECCI√ìN DE COLUMNAS
temps = df['temp_pasteurizacion']  # Retorna Series
subset = df[['id_lote', 'ph']]     # Retorna DataFrame (nota el [[ ]])

# 2. SELECCI√ìN POR ETIQUETA (.loc)
# Sintaxis: df.loc[filas, columnas]
primera_fila = df.loc[0]                    # Primera fila completa
ph_L002 = df.loc[1, 'ph']                   # Celda espec√≠fica: 6.6
rango = df.loc[0:2, 'temp_pasteurizacion']  # Filas 0-2, una columna

# 3. SELECCI√ìN POR POSICI√ìN (.iloc)
primera_celda = df.iloc[0, 0]     # 'L001'
subcuadro = df.iloc[0:2, 1:3]     # 2 filas √ó 2 columnas

# H√°bito profesional: siempre verifica tu selecci√≥n
print("Tipo de temps:", type(temps))  # <class 'pandas.core.series.Series'>
print("Tipo de subset:", type(subset)) # <class 'pandas.core.frame.DataFrame'>
\end{lstlisting}

\subsection{Filtrado Booleano (M√°scaras)}

El poder real de Pandas est√° en las \textbf{consultas vectorizadas}. No uses bucles \texttt{for} ‚Äî usa m√°scaras booleanas. Esta es la base de la eficiencia en pipelines de datos industriales.[web:23][web:26]

\begin{lstlisting}[language=Python, caption={Filtrado avanzado para control de calidad}]
import pandas as pd

# Cargar datos de producci√≥n
df = pd.read_csv('produccion_cafe_enero.csv')

# 1. CONSULTA SIMPLE: Lotes rechazados
rechazados = df[df['resultado'] == 'Rechazado']

# 2. CONSULTAS COMPUESTAS: Temperatura fuera de spec Y presi√≥n baja
# Rango de pasteurizaci√≥n: 72-76¬∞C, Presi√≥n m√≠nima: 2.5 bar
problemas_criticos = df[
    ((df['temp_C'] < 72) | (df['temp_C'] > 76)) &
    (df['presion_bar'] < 2.5)
]

# 3. FILTRO POR LISTA (isin): Solo l√≠neas L1 y L3
lineas_foco = df[df['linea'].isin(['L1', 'L3'])]

# 4. FILTRO POR STRING (contiene): Lotes de turno nocturno
nocturnos = df[df['id_lote'].str.contains('NOCHE')]

# 5. QUERY (sintaxis SQL-like)
# Nota: Solo funciona si nombres de columnas no tienen espacios
criticos = df.query('temp_C > 76 and resultado == "Rechazado"')

# H√°bito de ingeniero: Verificar resultados
print(f"Lotes rechazados: {len(rechazados)}")
print(f"Problemas cr√≠ticos: {len(problemas_criticos)}")
\end{lstlisting}

\begin{industrybox}{¬øPor qu√© las m√°scaras son la base de Data Engineering?}
En la industria, el filtrado no es solo para an√°lisis ‚Äî es para \textbf{alertas en tiempo real}. Imagina un sistema que:
\begin{enumerate}
    \item Carga datos cada 5 minutos desde sensores IoT
    \item Aplica una m√°scara booleana: \texttt{df[df['temp'] > 80]}
    \item Si encuentra resultados, env√≠a alerta SMS al supervisor
\end{enumerate}

Esto se ejecuta 24/7 sin intervenci√≥n humana. Un bucle \texttt{for} no ser√≠a confiable para esto.
\end{industrybox}

\begin{sciencebox}{Complejidad Computacional de M√°scaras}
Una m√°scara booleana \texttt{df['temp'] > 72} tiene complejidad \( O(n) \) donde \( n \) es el n√∫mero de filas. Internamente:
\begin{enumerate}
    \item Pandas delega la comparaci√≥n a NumPy (c√≥digo C optimizado)
    \item Se crea un array booleano en memoria del mismo tama√±o que la columna
    \item El filtrado \texttt{df[mask]} usa fancy indexing de NumPy
\end{enumerate}

Para un DataFrame de 1M filas, esto toma $\sim$10ms. Un bucle \texttt{for} equivalente tomar√≠a $\sim$2 segundos (200x m√°s lento).[web:23]
\end{sciencebox}

\begin{conceptbox}{Buena pr√°ctica \#2: Nombres descriptivos para filtros}
En lugar de:
\begin{lstlisting}[language=Python]
problemas = df[df['temp'] > 80]
\end{lstlisting}

Usa:
\begin{lstlisting}[language=Python]
alerta_temp_alta = df[df['temp_pasteurizacion'] > 80]
\end{lstlisting}

Cuando revises el c√≥digo en 6 meses (o un colega lo revise), sabr√°s inmediatamente qu√© representa esa variable.
\end{conceptbox}

\newpage

\section{Cap√≠tulo III: Limpieza de Datos ‚Äî Fail Fast en Producci√≥n}

\subsection{El Problema de los Tipos Incorrectos}

\begin{warningbox}{Tipo \texttt{object} = Peligro}
Si una columna num√©rica aparece como \texttt{dtype: object}, significa que Pandas la ley√≥ como texto. No podr√°s hacer operaciones matem√°ticas hasta convertirla.
\end{warningbox}

\textbf{Causas comunes}:
\begin{itemize}
    \item Un solo valor con texto ("Error", "N/A", "-") contamina toda la columna
    \item Formato de n√∫mero europeo: "3,14" en lugar de "3.14"
    \item Espacios en blanco: " 25.5 " no se convierte autom√°ticamente
\end{itemize}

\textbf{Este es el primer paso de Data Quality Engineering:} asegurar que cada columna tenga el \texttt{dtype} correcto desde la ingesti√≥n. Un ingeniero de datos profesional nunca deja pasar datos con tipos incorrectos a la siguiente etapa del pipeline.[web:23][web:26]

\begin{lstlisting}[language=Python, caption={Diagn√≥stico y correcci√≥n de tipos}]
import pandas as pd

df = pd.read_csv('sensores_planta.csv')

# 1. DIAGN√ìSTICO
print(df.dtypes)
print(df.info())  # Muestra tipos y valores no-nulos

# Ejemplo de salida problem√°tica:
# temperatura    object  ‚Üê ‚ö† Deber√≠a ser float64
# presion        object  ‚Üê ‚ö† Deber√≠a ser float64

# 2. INSPECCI√ìN MANUAL
print(df['temperatura'].unique())  # Ver valores √∫nicos
# Output: ['25.5', '26.1', 'Error', '24.8', ...]  ‚Üê "Error" causa el problema

# 3. CONVERSI√ìN FORZADA (errores ‚Üí NaN)
df['temperatura'] = pd.to_numeric(df['temperatura'], errors='coerce')
df['presion'] = pd.to_numeric(df['presion'], errors='coerce')

# 4. VERIFICACI√ìN
print(df.dtypes)
# temperatura    float64  ‚Üê ‚úì Corregido
# presion        float64  ‚Üê ‚úì Corregido

print(df['temperatura'].isna().sum())  # Contar cu√°ntos NaN se generaron

# H√°bito pro: Registrar el proceso de limpieza
print(f"Convertidos {df['temperatura'].isna().sum()} valores inv√°lidos a NaN")
\end{lstlisting}

\subsection{Tratamiento de Valores Faltantes}

En la industria alimenticia, \textbf{un dato faltante puede significar un fallo cr√≠tico}. No siempre es correcto rellenar con el promedio. La decisi√≥n debe basarse en el \textbf{conocimiento del dominio} y los requisitos de trazabilidad (ISO 22000, HACCP).[web:23]

\begin{table}[h]
\centering
\begin{tabular}{@{}lp{6cm}p{5cm}@{}}
\toprule
\textbf{M√©todo} & \textbf{Cu√°ndo usarlo} & \textbf{Riesgo} \\
\midrule
\texttt{fillna(0)} & Contadores (eventos) & 0 puede ser v√°lido en agro \\
\texttt{ffill()} & Series temporales (sensores) & Oculta fallos prolongados \\
\texttt{interpolate()} & Datos continuos (temperatura) & Inventa datos inexistentes \\
\texttt{dropna()} & QA cr√≠tico & Pierdes informaci√≥n \\
\bottomrule
\end{tabular}
\caption{Estrategias de imputaci√≥n de datos faltantes}
\end{table}

\begin{lstlisting}[language=Python, caption={Imputaci√≥n contextual para sensores}]
import pandas as pd

df = pd.read_csv('temperatura_camara_fria.csv', parse_dates=['timestamp'])
df = df.set_index('timestamp')

# CASO 1: Interpolaci√≥n limitada (m√°ximo 2 valores consecutivos)
# Si faltan >2 valores, algo fall√≥ y no deber√≠amos inventar datos
df['temp'] = df['temp'].interpolate(method='time', limit=2)

# CASO 2: Forward fill con l√≠mite temporal
# Rellenar con el √∫ltimo valor conocido, pero solo por 10 minutos
df['humedad'] = df['humedad'].fillna(method='ffill', limit=20)  # 20 registros = 10 min

# CASO 3: Marcar como fallo en lugar de imputar
df['sensor_falla'] = df['temp'].isna()  # Columna booleana de alertas

# CASO 4: Eliminar filas con datos cr√≠ticos faltantes
df_limpio = df.dropna(subset=['ph', 'acidez'])  # Solo si faltan variables cr√≠ticas

# H√°bito pro: Reporte de imputaciones
print(f"Imputados por interpolaci√≥n: {df['temp'].isna().sum()}")
print(f"Sensores con fallas: {df['sensor_falla'].sum()}")
\end{lstlisting}

\begin{industrybox}{Trazabilidad: Nunca pierdas el origen del dato}
En sistemas certificados (HACCP, FDA), debes documentar:
\begin{itemize}
    \item ¬øCu√°ntos valores faltaban originalmente?
    \item ¬øQu√© m√©todo de imputaci√≥n usaste?
    \item ¬øQu√© umbrales configuraste?
\end{itemize}

Tu c√≥digo de limpieza debe ser \textbf{auditable}. Un auditor debe poder reproducir exactamente el mismo resultado.
\end{industrybox}

\subsection{Detecci√≥n de Outliers}

\begin{lstlisting}[language=Python, caption={Detecci√≥n estad√≠stica de anomal√≠as}]
import pandas as pd
import numpy as np

df = pd.read_csv('temperatura_pasteurizacion.csv')

# M√âTODO 1: Rango intercuart√≠lico (IQR) ‚Äî Robusto a valores extremos
Q1 = df['temp'].quantile(0.25)
Q3 = df['temp'].quantile(0.75)
IQR = Q3 - Q1

limite_inferior = Q1 - 1.5 * IQR
limite_superior = Q3 + 1.5 * IQR

outliers = df[(df['temp'] < limite_inferior) | (df['temp'] > limite_superior)]
print(f"Detectados {len(outliers)} outliers estad√≠sticos")

# M√âTODO 2: Z-score (asume distribuci√≥n normal)
mean = df['temp'].mean()
std = df['temp'].std()
df['z_score'] = (df['temp'] - mean) / std

# Outliers: |z| > 3 (regla de 3 sigmas)
outliers_zscore = df[np.abs(df['z_score']) > 3]

# M√âTODO 3: L√≠mites f√≠sicos (conocimiento del dominio)
# La temperatura de pasteurizaci√≥n NUNCA puede ser > 100¬∞C
errores_sensor = df[df['temp'] > 100]
df.loc[df['temp'] > 100, 'temp'] = np.nan  # Marcar como faltante

print(f"Errores f√≠sicos detectados: {len(errores_sensor)}")

# H√°bito pro: Guardar anomal√≠as para auditor√≠a
df['es_outlier'] = np.abs(df['z_score']) > 3
print(df['es_outlier'].value_counts())
\end{lstlisting}

\begin{conceptbox}{Jerarqu√≠a de detecci√≥n de anomal√≠as (Data Quality Engineering)}
\begin{enumerate}
    \item \textbf{L√≠mites f√≠sicos} (temperatura > 100¬∞C = error sensor)
    \item \textbf{Reglas de negocio} (pH fuera de rango de pasteurizaci√≥n)
    \item \textbf{An√°lisis estad√≠stico} (IQR, Z-score)
\end{enumerate}

Siempre aplica en este orden. Los l√≠mites f√≠sicos son los m√°s confiables.
\end{conceptbox}

\newpage

\section{Cap√≠tulo IV: GroupBy ‚Äî El Motor de Agregaci√≥n Industrial}

\subsection{El Paradigma Split-Apply-Combine}

\texttt{.groupby()} es la operaci√≥n m√°s importante en Pandas. Implementa el patr√≥n \textit{split-apply-combine}, que es el coraz√≥n de los \textbf{pipelines de agregaci√≥n} en Data Engineering.[web:23][web:26]

\begin{center}
\begin{tikzpicture}[
    node distance=2cm,
    box/.style={rectangle, draw, minimum width=2.5cm, minimum height=1.2cm, align=center, rounded corners, thick},
    arrow/.style={->, ultra thick, >=stealth}
]
    \node[box, fill=blue!20] (original) {DataFrame\\Original};
    \node[box, fill=green!20, below=of original] (split) {SPLIT\\(Dividir por grupos)};
    \node[box, fill=orange!20, below=of split] (apply) {APPLY\\(Aplicar funci√≥n)};
    \node[box, fill=violet!20, below=of apply] (combine) {COMBINE\\(Combinar resultados)};

    \draw[arrow] (original) -- (split);
    \draw[arrow] (split) -- (apply);
    \draw[arrow] (apply) -- (combine);
\end{tikzpicture}
\end{center}

\textbf{Aplicaci√≥n industrial}: Generar reportes de KPIs (Key Performance Indicators), calcular OEE (Overall Equipment Effectiveness) y detectar desviaciones de rendimiento por l√≠nea o turno.[web:23]

\begin{lstlisting}[language=Python, caption={An√°lisis de productividad por l√≠nea (KPIs industriales)}]
import pandas as pd

# Dataset: 2000 lotes de caf√© procesados en enero
df = pd.read_csv('produccion_cafe_enero.csv', parse_dates=['timestamp_inicio', 'timestamp_fin'])

# Calcular duraci√≥n de cada lote
df['duracion_min'] = (df['timestamp_fin'] - df['timestamp_inicio']).dt.total_seconds() / 60

# AGREGACI√ìN 1: Productividad por l√≠nea (KPIs)
productividad = df.groupby('linea').agg({
    'kg_procesados': 'sum',         # Total de kilos
    'duracion_min': ['mean', 'std'], # Estad√≠sticas de tiempo
    'id_lote': 'count'               # Cantidad de lotes
}).round(2)

productividad.columns = ['_'.join(col).strip() for col in productividad.columns]  # Nombres limpios
print(productividad)

# AGREGACI√ìN 2: Rechazos por turno (Matriz de control)
rechazos = df.groupby(['turno', 'resultado']).size().unstack(fill_value=0)
print(rechazos)

# AGREGACI√ìN 3: M√∫ltiples estad√≠sticas con nombres personalizados
stats = df.groupby('linea')['duracion_min'].agg(
    media='mean',
    desviacion='std',
    minimo='min',
    maximo='max'
).round(1)

print(stats)

# H√°bito pro: Verificar resultados
print(f"Total de lotes procesados: {len(df)}")
print(f"Total de kg procesados: {df['kg_procesados'].sum():,.0f}")
\end{lstlisting}

\begin{industrybox}{OEE (Overall Equipment Effectiveness) con GroupBy}
El OEE es el KPI rey de la industria 4.0. Se calcula as√≠:
\[
\text{OEE} = \text{Disponibilidad} \times \text{Rendimiento} \times \text{Calidad}
\]

Con GroupBy puedes calcularlo por l√≠nea, turno o mes:
\begin{lstlisting}[language=Python]
oee = df.groupby('linea').apply(lambda x: calcular_oee(x))
\end{lstlisting}
\end{industrybox}

\subsection{GroupBy con Transformaciones}

A veces no quieres reducir el DataFrame, sino \textbf{agregar columnas calculadas por grupo} que se mantienen alineadas con el √≠ndice original. Esto es esencial para \textbf{benchmarking} y normalizaci√≥n.[web:23]

\begin{lstlisting}[language=Python, caption={Normalizaci√≥n y ranking por grupo}]
import pandas as pd

df = pd.read_csv('lotes_produccion.csv')

# CASO 1: Calcular % de productividad de cada lote respecto a su l√≠nea
df['kg_promedio_linea'] = df.groupby('linea')['kg_procesados'].transform('mean')
df['performance_relativo'] = (df['kg_procesados'] / df['kg_promedio_linea']) * 100

# CASO 2: Ranking dentro de cada turno (Top performers)
df['ranking_turno'] = df.groupby('turno')['kg_procesados'].rank(ascending=False, method='min')

# CASO 3: Detectar lotes at√≠picos (> 2 std de su grupo)
df['media_linea'] = df.groupby('linea')['duracion_min'].transform('mean')
df['std_linea'] = df.groupby('linea')['duracion_min'].transform('std')
df['es_atipico'] = np.abs(df['duracion_min'] - df['media_linea']) > (2 * df['std_linea'])

# Verificaci√≥n
print(df[['linea', 'kg_procesados', 'performance_relativo', 'es_atipico']].head())
print(f"Lotes at√≠picos detectados: {df['es_atipico'].sum()}")
\end{lstlisting}

\begin{conceptbox}{Transform vs Agg: La diferencia clave}
\begin{itemize}
    \item \texttt{.agg()} ‚Üí Reduce filas (1 n√∫mero por grupo)
    \item \texttt{.transform()} ‚Üí Mantiene todas las filas (1 valor por fila original)
\end{itemize}

\textit{Ejemplo:} Si tienes 100 lotes en la l√≠nea L1:
\begin{itemize}
    \item \texttt{agg('mean')} ‚Üí 1 valor (la media de L1)
    \item \texttt{transform('mean')} ‚Üí 100 valores (la media de L1 repetida 100 veces)
\end{itemize}
\end{conceptbox}

\begin{sciencebox}{Complejidad de GroupBy}
Internamente, \texttt{.groupby()} usa un algoritmo de hashing para agrupar filas:
\begin{enumerate}
    \item Calcula hash de cada valor en la columna de agrupaci√≥n: \( O(n) \)
    \item Ordena los √≠ndices por hash: \( O(n \log n) \)
    \item Aplica funci√≥n a cada grupo: \( O(n) \)
\end{enumerate}

Complejidad total: \( O(n \log n) \). Para 1M filas, esto toma $\sim$100ms en un CPU moderno.

\textbf{Comparaci√≥n}: Un bucle manual con diccionarios tomar√≠a $\sim$5 segundos (50x m√°s lento).[web:23]
\end{sciencebox}

\newpage
\section{Cap√≠tulo V: Series Temporales ‚Äî El Coraz√≥n de la Industria 4.0}

\subsection{Datetime como Index}

En la industria, \textbf{el tiempo es el √≠ndice natural} de los datos. Convertir el DataFrame a √≠ndice temporal desbloquea operaciones avanzadas como \textbf{resampling} y \textbf{rolling windows}, esenciales para SPC (Statistical Process Control) y detecci√≥n de anomal√≠as en tiempo real.[web:23][web:31]

\textbf{¬øPor qu√© es cr√≠tico?} Los sistemas SCADA y sensores IoT generan datos timestamped. Sin √≠ndice temporal, pierdes la capacidad de hacer slicing por rangos de fecha y an√°lisis de tendencias.[web:25][web:28]

\begin{lstlisting}[language=Python, caption={Configurar √≠ndice temporal (est√°ndar industrial)}]
import pandas as pd

# Cargar datos de sensor con timestamps
df = pd.read_csv('temperatura_camara.csv')

# PASO 1: Convertir columna a datetime (maneja formatos mixtos)
df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')

# PASO 2: Establecer como √≠ndice
df_ts = df.set_index('timestamp')

# PASO 3: Ordenar por tiempo (¬°importante!)
df_ts = df_ts.sort_index()

# PASO 4: Verificar frecuencia (opcional, pero profesional)
print("Frecuencia:", df_ts.index.freq)
print("Rango temporal:", df_ts.index.min(), "‚Üí", df_ts.index.max())

# Ahora puedes hacer selecci√≥n por rangos de fecha:
enero = df_ts['2026-01-01':'2026-01-31']
primera_semana = df_ts['2026-01-01':'2026-01-07 23:59']

# H√°bito pro: Verificar integridad temporal
print(f"Total de registros: {len(df_ts)}")
print(f"Gap m√°ximo: {(df_ts.index[1:] - df_ts.index[:-1]).max()}")
\end{lstlisting}

\begin{warningbox}{Problema com√∫n: Timezones}
Si tus sensores est√°n en UTC pero tu planta en Colombia (COT), usa:
\begin{lstlisting}[language=Python]
df_ts.index = df_ts.index.tz_localize('UTC').tz_convert('America/Bogota')
\end{lstlisting}
\end{warningbox}

\subsection{Resampling ‚Äî Cambiar la Frecuencia para Reportes}

\begin{conceptbox}{Resampling vs Rolling}
\begin{itemize}
    \item \textbf{Resample}: Cambia la frecuencia temporal. Ejemplo: datos cada 30 seg ‚Üí promedio diario.
    \item \textbf{Rolling}: Ventana deslizante. Mantiene la frecuencia original pero suaviza con promedios m√≥viles.
\end{itemize}
\end{conceptbox}

\begin{lstlisting}[language=Python, caption={Resampling para reportes diarios y alertas}]
import pandas as pd

# Datos de temperatura cada 30 segundos
df = pd.read_csv('temp_pasteurizacion.csv', parse_dates=['timestamp'], index_col='timestamp')

# RESAMPLE 1: Promedio diario (reporte ejecutivo)
temp_diaria = df['temperatura'].resample('D').agg(['mean', 'min', 'max'])

# RESAMPLE 2: M√°ximo por hora (detecci√≥n de picos)
temp_horaria_max = df['temperatura'].resample('H').max()

# RESAMPLE 3: M√∫ltiples agregaciones para dashboard
stats_diarias = df.resample('D').agg({
    'temperatura': ['mean', 'min', 'max', 'std'],
    'presion': 'mean'
}).round(2)

# RESAMPLE 4: Contar eventos por turno (8 horas)
eventos_turno = df.resample('8H').size()

# Verificaci√≥n
print("Estad√≠sticas diarias:")
print(stats_diarias.head())
print(f"Eventos por turno: {eventos_turno.sum()}")
\end{lstlisting}

\begin{industrybox}{Aplicaci√≥n: Alertas autom√°ticas por d√≠a}
\begin{lstlisting}[language=Python]
# Si el m√°ximo diario supera 78¬∞C, generar alerta
alertas = temp_diaria['max'] > 78
print(f"D√≠as con alerta: {alertas.sum()}")
\end{lstlisting}
\end{industrybox}

\subsection{Rolling Windows ‚Äî Suavizar Ruido para Control de Procesos}

\begin{lstlisting}[language=Python, caption={Ventanas m√≥viles para SPC (Statistical Process Control)}]
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('temperatura_real_time.csv', parse_dates=['timestamp'], index_col='timestamp')

# Media m√≥vil de 10 minutos (window=20 si los datos son cada 30 seg)
df['temp_suavizada'] = df['temperatura'].rolling(window=20, center=True).mean()

# Desviaci√≥n est√°ndar m√≥vil (detectar variabilidad del proceso)
df['temp_std_movil'] = df['temperatura'].rolling(window=20, center=True).std()

# L√≠mites de control (reglas Shewhart ¬±3œÉ)
media_global = df['temperatura'].mean()
std_global = df['temperatura'].std()
df['limite_superior'] = media_global + 3 * std_global
df['limite_inferior'] = media_global - 3 * std_global

# Detectar derivas: si la std m√≥vil supera 2¬∞C, el proceso est√° inestable
df['proceso_inestable'] = df['temp_std_movil'] > 2.0

# Visualizaci√≥n SPC
plt.figure(figsize=(14, 8))
plt.plot(df.index, df['temperatura'], alpha=0.5, label='Datos crudos', linewidth=0.8)
plt.plot(df.index, df['temp_suavizada'], linewidth=2, label='Media m√≥vil 10 min')
plt.axhline(media_global, color='green', linestyle='--', label='Media global')
plt.axhline(df['limite_superior'].iloc[0], color='red', linestyle='--', alpha=0.7, label='L√≠mite superior ¬±3œÉ')
plt.axhline(df['limite_inferior'].iloc[0], color='red', linestyle='--', alpha=0.7, label='L√≠mite inferior')
plt.fill_between(df.index, df['limite_inferior'].iloc[0], df['limite_superior'].iloc[0],
                 alpha=0.1, color='red')
plt.legend()
plt.title('Control Estad√≠stico de Procesos - Temperatura Pasteurizaci√≥n')
plt.ylabel('Temperatura (¬∞C)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('spc_temperatura.png', dpi=150, bbox_inches='tight')
plt.show()

# Reporte de control
print(f"Procesos inestables detectados: {df['proceso_inestable'].sum()}")
print(f"Porcentaje fuera de control: {df['proceso_inestable'].mean()*100:.1f}%")
\end{lstlisting}

\begin{sciencebox}{SPC (Statistical Process Control) con Pandas}
Las ventanas m√≥viles implementan las \textbf{cartas de control} de Shewhart:
\[
\text{L√≠mites} = \bar{x} \pm 3\sigma
\]
Donde $\bar{x}$ es la media hist√≥rica y $\sigma$ la desviaci√≥n est√°ndar. Esto es exactamente lo que usan las f√°bricas modernas para monitoreo 24/7.
\end{sciencebox}

\newpage

\section{Cap√≠tulo VI: Merge y Trazabilidad ‚Äî Data Integration Engineering}

\subsection{El Problema de la Trazabilidad}

En agroindustria alimenticia, la trazabilidad es \textbf{un requisito legal} (HACCP, ISO 22000, FDA 21 CFR Part 11). Debes poder responder en menos de 4 horas durante un recall:

\begin{itemize}
    \item ¬øQu√© clientes recibieron lotes de un proveedor contaminado?
    \item ¬øQu√© lotes fueron procesados por un operario espec√≠fico en una fecha?
    \item ¬øQu√© materia prima se us√≥ en un lote con defecto?
\end{itemize}

Esto requiere \textbf{Data Integration}: cruzar m√∫ltiples tablas heterog√©neas usando \texttt{pd.merge()}. En Data Engineering, esto se conoce como \textbf{ETL de enriquecimiento}.[web:23][web:26]

\subsection{Tipos de Merge}

\begin{table}[h]
\centering
\begin{tabular}{@{}lp{8cm}@{}}
\toprule
\textbf{Tipo} & \textbf{Comportamiento} \\
\midrule
\texttt{inner} & Solo filas con match en ambas tablas (intersecci√≥n) \\
\texttt{left} & Todas las filas de la tabla izquierda + matches de la derecha \\
\texttt{right} & Todas las filas de la tabla derecha + matches de la izquierda \\
\texttt{outer} & Todas las filas de ambas tablas (uni√≥n) \\
\bottomrule
\end{tabular}
\caption{Tipos de merge en Pandas}
\end{table}

\textbf{¬øCu√°l usar en trazabilidad?} Siempre \texttt{left} cuando sigues la cadena de valor (lotes ‚Üí pruebas ‚Üí despachos).[web:23]

\begin{lstlisting}[language=Python, caption={Caso HACCP: Pipeline completo de trazabilidad}]
import pandas as pd

# PASO 0: Verificar esquemas antes de merge (h√°bito profesional)
print("=== ESQUEMAS ===")
lotes = pd.read_csv('lotes_producidos.csv')
pruebas = pd.read_csv('pruebas_laboratorio.csv')
despachos = pd.read_csv('despachos.csv')

print("Lotes:", lotes['id_lote'].dtype, lotes['id_lote'].head())
print("Pruebas:", pruebas['id_lote'].dtype, pruebas['id_lote'].head())
print("Despachos:", despachos['id_lote'].dtype, despachos['id_lote'].head())

# Estandarizar tipos de clave (cr√≠tico)
for df_merge in [lotes, pruebas, despachos]:
    df_merge['id_lote'] = df_merge['id_lote'].astype(str)

print("\n=== POST ESTANDARIZACI√ìN ===")
print("Matches posibles:", len(set(lotes['id_lote']) & set(pruebas['id_lote'])))

# PASO 1: Primera uni√≥n (lotes + pruebas)
lotes_con_qa = pd.merge(lotes, pruebas, on='id_lote', how='left')

# PASO 2: Segunda uni√≥n (agregar despachos)
trazabilidad_completa = pd.merge(lotes_con_qa, despachos, on='id_lote', how='left')

# PASO 3: Identificar lotes problem√°ticos (pH < 4.3 O acidez > 0.8)
lotes_problematicos = trazabilidad_completa[
    (trazabilidad_completa['ph'] < 4.3) |
    (trazabilidad_completa['acidez'] > 0.8)
]

# PASO 4: KPIs del recall
print(f"\n=== REPORT RECALL ===")
print(f"Lotes problem√°ticos: {len(lotes_problematicos)}")
print(f"Clientes afectados √∫nicos: {lotes_problematicos['cliente'].nunique()}")
print(f"Total kg afectados: {lotes_problematicos['kg'].sum():.0f}")

# PASO 5: Exportar para reporte de recall (formato auditable)
lotes_problematicos[['id_lote', 'cliente', 'destino', 'fecha_envio', 'ph', 'acidez']].to_csv(
    'recall_lista_clientes.csv', index=False
)

# PASO 6: Resumen ejecutivo
resumen_recall = lotes_problematicos.groupby('cliente').agg({
    'id_lote': 'count',
    'kg': 'sum'
}).round(0)
resumen_recall.to_csv('resumen_recall_clientes.csv')
print("\nClientes m√°s afectados:")
print(resumen_recall.sort_values('kg', ascending=False))
\end{lstlisting}

\begin{industrybox}{Pipeline de Trazabilidad Industrial Completo}
\begin{enumerate}
    \item \textbf{Ingesti√≥n}: Cargar tablas desde ERP/LIMS/SCADA
    \item \textbf{Data Quality}: Validar claves y tipos
    \item \textbf{Enriquecimiento}: Merge secuencial (left joins)
    \item \textbf{Alerts}: Filtrar por reglas de negocio
    \item \textbf{Reporting}: Exportar para auditor√≠a
\end{enumerate}

Este patr√≥n se ejecuta diariamente en plantas certificadas.
\end{industrybox}

\begin{warningbox}{Error Com√∫n: Claves con tipos diferentes}
Si \texttt{lotes['id\_lote']} es string y \texttt{pruebas['id\_lote']} es int, el merge fallar√° silenciosamente (0 matches).

\textbf{Soluci√≥n profesional implementada arriba}: Siempre verificar y estandarizar tipos antes de merge.
\end{warningbox}

\begin{conceptbox}{MultiIndex para Trazabilidad Avanzada}
Para cruces m√°s complejos (ej: lote + fecha + proveedor), usa:
\begin{lstlisting}[language=Python]
df.set_index(['id_lote', 'fecha']).join(otra_tabla.set_index(['id_lote', 'fecha']))
\end{lstlisting}
\end{conceptbox}

\newpage

\section{Cap√≠tulo VII: Ingenier√≠a de Caracter√≠sticas ‚Äî Feature Engineering Industrial}

\subsection{Creaci√≥n de Columnas Derivadas}

La \textbf{ingenier√≠a de caracter√≠sticas} (Feature Engineering) convierte datos crudos en variables predictivas √∫tiles. En la industria, esto significa crear KPIs, indicadores de control y variables para Machine Learning.[web:23][web:26]

\begin{lstlisting}[language=Python, caption={Feature Engineering para KPIs y predicci√≥n}]
import pandas as pd
import numpy as np

df = pd.read_csv('produccion_diaria.csv', parse_dates=['fecha', 'hora_inicio', 'hora_fin'])

# 1. Duraci√≥n de proceso (timedelta a minutos) ‚Äî Base para OEE
df['duracion_min'] = (df['hora_fin'] - df['hora_inicio']).dt.total_seconds() / 60

# 2. Rendimiento (kg/hora) ‚Äî KPI principal
df['rendimiento_kgh'] = df['kg_producados'] / (df['duracion_min'] / 60)

# 3. OEE simplificado (Disponibilidad √ó Rendimiento √ó Calidad)
df['disponibilidad'] = df['duracion_real'] / df['duracion_programada']
df['calidad'] = (df['kg_producados'] / df['kg_programados']) * 100
df['oee'] = df['disponibilidad'] * (df['rendimiento_kgh'] / df['rendimiento_ideal']) * df['calidad'] / 100

# 4. Categorizaci√≥n de turnos (vectorizado, NO funci√≥n)
df['turno'] = pd.cut(df['hora_inicio'].dt.hour,
                     bins=[0, 6, 14, 22, 24],
                     labels=['Noche', 'Ma√±ana', 'Tarde', 'Noche'],
                     right=False)

# 5. Features temporales avanzadas
df['dia_semana'] = df['fecha'].dt.day_name()
df['semana'] = df['fecha'].dt.isocalendar().week
df['es_fin_semana'] = df['fecha'].dt.weekday >= 5
df['mes'] = df['fecha'].dt.month

# 6. Features de interacci√≥n (producto de variables)
df['linea_x_turno'] = df['linea'].astype('category').cat.codes * df['turno'].astype('category').cat.codes

# Verificaci√≥n de nuevas features
print(df[['oee', 'rendimiento_kgh', 'turno', 'es_fin_semana']].describe())
\end{lstlisting}

\begin{industrybox}{OEE como Feature Principal}
El OEE es el KPI rey de Industria 4.0. Valores t√≠picos:
\begin{itemize}
    \item \textbf{Mundial:} 85%
    \item \textbf{Excelente:} >90%
    \item \textbf{Mala se√±al:} <70%
\end{itemize}

Tu feature engineering debe priorizar variables que impacten el OEE.
\end{industrybox}

\begin{conceptbox}{Vectorizado vs Apply: Rendimiento 100x}
\begin{lstlisting}[language=Python]
# MAL (lento)
df['turno'] = df['hora'].apply(clasificar_turno)

# BIEN (r√°pido)
df['turno'] = pd.cut(df['hora'], bins=[0,6,14,22,24], labels=['N','M','T','N'])
\end{lstlisting}
\end{conceptbox}

\subsection{Discretizaci√≥n (Binning) y Encoding Categ√≥rico}

\begin{lstlisting}[language=Python, caption={Feature Engineering categ√≥rico para ML}]
import pandas as pd
import numpy as np

df = pd.read_csv('analisis_ph.csv')

# 1. Binning de variables continuas (para reglas de negocio)
bins_ph = [0, 4.3, 6.5, 7.0, 14]
labels_ph = ['Cr√≠tico_Bajo', '√Åcido', 'Neutro_Bajo', 'Neutro_Alto']
df['categoria_ph'] = pd.cut(df['ph'], bins=bins_ph, labels=labels_ph, include_lowest=True)

# 2. Binning de temperatura con l√≠mites de spec
bins_temp = [0, 71, 74, 77, 100]
labels_temp = ['Bajo_Spec', 'Optimo_Bajo', 'Optimo_Alto', 'Sobre_Spec']
df['zona_temp'] = pd.cut(df['temp'], bins=bins_temp, labels=labels_temp)

# 3. Encoding num√©rico para ML (category codes)
df['linea_encoded'] = df['linea'].astype('category').cat.codes
df['operario_encoded'] = df['operario'].astype('category').cat.codes

# 4. Features polin√≥micas (interacciones cuadr√°ticas)
df['temp_cuadrado'] = df['temp'] ** 2  # Efecto no lineal
df['ph_temp_interaccion'] = df['ph'] * df['temp']

# 5. Binning cuant√≠lico (distribuci√≥n uniforme)
df['rendimiento_quintil'] = pd.qcut(df['rendimiento'], q=5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])

# Ver nuevas features
print(df[['categoria_ph', 'zona_temp', 'rendimiento_quintil', 'linea_encoded']].head())
print("\nDistribuci√≥n de quintiles:")
print(df['rendimiento_quintil'].value_counts())
\end{lstlisting}

\begin{sciencebox}{Por qu√© el Binning es poderoso en ML industrial}
\begin{itemize}
    \item \textbf{Interpretabilidad}: "√ìptimo Alto" es m√°s legible que "75.3¬∞C"
    \item \textbf{Robustez}: Reduce ruido en variables continuas
    \item \textbf{Normalizaci√≥n}: Quintiles crean distribuciones uniformes
\end{itemize}

Usa \texttt{pd.cut()} para bins fijos (reglas de negocio) y \texttt{pd.qcut()} para bins por percentiles.
\end{sciencebox}

\begin{warningbox}{¬°Cuidado con el Leakage!}
Nunca uses informaci√≥n del futuro para crear features de entrenamiento:
\begin{lstlisting}[language=Python]
# MAL (data leakage)
df['resultado_lag1'] = df['resultado'].shift(-1)

# BIEN
df['resultado_lag1'] = df['resultado'].shift(1)  # Solo pasado
\end{lstlisting}
\end{warningbox}

\newpage

\section{Cap√≠tulo VIII: √âtica y Calidad de Datos}

\begin{ethicsbox}{Responsabilidad en la Limpieza de Datos}
Cada decisi√≥n de limpieza altera la realidad registrada. En la industria alimenticia, esto tiene implicaciones legales y de salud p√∫blica.

\textbf{Principios √©ticos}:
\begin{enumerate}
    \item \textbf{Trazabilidad}: Guardar dataset original sin modificar (\texttt{raw/})
    \item \textbf{Documentaci√≥n}: Registrar cada transformaci√≥n en un log
    \item \textbf{Transparencia}: Reportar cu√°ntas filas se eliminaron y por qu√©
    \item \textbf{Sesgo de imputaci√≥n}: No ocultar fallos sistem√°ticos rellenando con promedios
\end{enumerate}
\end{ethicsbox}

\subsection{Pipeline de Limpieza Documentado}

\begin{lstlisting}[language=Python, caption={Pipeline con logging}]
import pandas as pd
import logging

# Configurar logging
logging.basicConfig(filename='limpieza.log', level=logging.INFO)

def limpiar_dataset(path_entrada, path_salida):
    # Cargar datos crudos
    df = pd.read_csv(path_entrada)
    filas_originales = len(df)
    logging.info(f"Dataset cargado: {filas_originales} filas")

    # 1. Eliminar duplicados
    df = df.drop_duplicates()
    duplicados = filas_originales - len(df)
    logging.info(f"Duplicados eliminados: {duplicados}")

    # 2. Convertir tipos
    df['temperatura'] = pd.to_numeric(df['temperatura'], errors='coerce')
    nulos_generados = df['temperatura'].isna().sum()
    logging.info(f"Valores no num√©ricos convertidos a NaN: {nulos_generados}")

    # 3. Eliminar outliers
    Q1 = df['temperatura'].quantile(0.25)
    Q3 = df['temperatura'].quantile(0.75)
    IQR = Q3 - Q1
    df_limpio = df[
        (df['temperatura'] >= Q1 - 1.5*IQR) &
        (df['temperatura'] <= Q3 + 1.5*IQR)
    ]
    outliers = len(df) - len(df_limpio)
    logging.info(f"Outliers eliminados: {outliers}")

    # Guardar dataset limpio
    df_limpio.to_csv(path_salida, index=False)
    logging.info(f"Dataset final: {len(df_limpio)} filas guardadas en {path_salida}")

    return df_limpio

# Ejecutar
df_limpio = limpiar_dataset('data/raw/sensores.csv', 'data/processed/sensores_clean.csv')
\end{lstlisting}

\newpage
\section{Cap√≠tulo IX: Talleres Pr√°cticos ‚Äî Data Engineering Real}

\subsection{Taller 1: Dashboard de Productividad (OEE Analysis)}

\begin{industrybox}{Caso: Planta de Procesamiento de Caf√© ‚Äî World Coffee Corp}
Tienes 2000 lotes procesados en enero en 3 l√≠neas autom√°ticas (L1, L2, L3). La gerencia ejecutiva necesita identificar cuellos de botella y mejorar OEE.

\textbf{KPIs requeridos}: Rendimiento (kg/h), OEE, lotes at√≠picos.
\end{industrybox}

\textbf{Dataset}: \texttt{data/raw/produccion\_cafe\_enero.csv}

\textbf{Entregables obligatorios}:
\begin{enumerate}
    \item Calcular duraci√≥n promedio por l√≠nea y turno
    \item Crear feature \texttt{rendimiento\_kgh} = kg / horas
    \item Identificar el turno m√°s lento (mayor duraci√≥n media)
    \item Detectar lotes at√≠picos (duraci√≥n > media + 2œÉ por l√≠nea)
    \item \textbf{Gr√°fico}: Boxplot de rendimiento por l√≠nea
    \item \textbf{Tabla resumen}: L√≠derboard de l√≠neas (OEE estimado)
    \item \textbf{Exportar}: \texttt{analisis\_productividad.xlsx} con 3 hojas
\end{enumerate}

\textbf{Criterio de √©xito}: Tabla con OEE >85\% para la mejor l√≠nea.

\subsection{Taller 2: Sistema de Alertas SPC (Statistical Process Control)}

\begin{industrybox}{Caso: Pasteurizaci√≥n de Leche ‚Äî DairyTech LATAM}
Sensor registra temperatura cada 30 segundos durante 7 d√≠as. Debes implementar control estad√≠stico de procesos y generar alertas autom√°ticas.

\textbf{Specs}: Temperatura debe mantenerse [72-76¬∞C]. Desviaciones >2¬∞C = proceso inestable.
\end{industrybox}

\textbf{Dataset}: \texttt{data/raw/temperatura\_pasteurizacion\_semana.csv}

\textbf{Entregables obligatorios}:
\begin{enumerate}
    \item Convertir \texttt{timestamp} a √≠ndice datetime y resamplear a 10 minutos
    \item Calcular media m√≥vil (20 puntos) y desviaci√≥n m√≥vil
    \item Implementar l√≠mites de control (¬±3œÉ de la media global)
    \item Detectar y contar periodos inestables (>2¬∞C std m√≥vil por >30 min)
    \item \textbf{Gr√°fico SPC}: L√≠nea cruda + suavizada + l√≠mites de control
    \item \textbf{Alerta}: CSV con timestamps de inestabilidad
    \item \textbf{Reporte}: \% tiempo en control
\end{enumerate}

\textbf{Criterio de √©xito}: Menos del 5\% del tiempo fuera de control.

\subsection{Taller 3: Pipeline de Recall HACCP (Data Integration)}

\begin{industrybox}{Caso HACCP: Recall Urgente ‚Äî Lote L20260115\_042}
INVIMA orden√≥ recall inmediato del lote contaminado L20260115\_042. Tienes 4 horas para generar lista completa de clientes afectados.

\textbf{Requisitos legales}: Trazabilidad completa en <4 horas (Reglamento UE 178/2002).
\end{industrybox}

\textbf{Datasets} (\texttt{data/raw/}):
\begin{itemize}
    \item \texttt{lotes\_producidos.csv} (id\_lote, fecha, linea, kg, proveedor)
    \item \texttt{pruebas\_microbiologia.csv} (id\_lote, ph, coliformes, resultado)
    \item \texttt{despachos\_clientes.csv} (id\_lote, cliente, direccion, fecha\_envio)
\end{itemize}

\textbf{Entregables obligatorios}:
\begin{enumerate}
    \item \textbf{Pipeline merge}: 3 tablas ‚Üí tabla trazabilidad completa (left joins)
    \item Verificar y estandarizar tipos de \texttt{id\_lote} antes de merge
    \item Filtrar lotes con \texttt{resultado == 'Contaminado'} o \texttt{coliformes > 10}
    \item Generar \texttt{recall\_clientes.csv}: cliente, direccion, kg\_afectados, fecha\_envio
    \item \textbf{Resumen ejecutivo}: Tabla agregada por cliente (kg totales)
    \item \textbf{Log de calidad}: Reporte de completitud del merge (\% matches)
    \item \textbf{Visualizaci√≥n}: Sankey diagram clientes ‚Üí kg afectados
\end{enumerate}

\textbf{Criterio de √©xito}: 100\% trazabilidad del lote contaminado.

\subsection{Taller 4: Feature Engineering Predictivo (Bonus Avanzado)}

\begin{industrybox}{Caso: Predicci√≥n de Fallos ‚Äî Mantenimiento Predictivo}
Crear features para modelo que prediga lotes con alto riesgo de rechazo.

\textbf{Target}: \texttt{resultado == 'Rechazado'}
\end{industrybox}

\textbf{Tareas bonus (+20 puntos)}:
\begin{enumerate}
    \item Crear 10 features nuevas (lags, rolling stats, interacciones)
    \item Binning de variables continuas (ph, temp, rendimiento)
    \item Encoding categ√≥rico (l√≠nea, proveedor, turno)
    \item Guardar \texttt{X\_train.csv}, \texttt{y\_train.csv} listo para scikit-learn
\end{enumerate}

\subsection{Instrucciones de Entrega}

\textbf{Formato notebook √∫nico} \texttt{talleres\_semana03.ipynb} con:
\begin{itemize}
    \item Secci√≥n por taller claramente marcada
    \item C√≥digo comentado + explicaciones
    \item Visualizaciones profesionales (t√≠tulos, leyendas, colores institucionales)
    \item Todos los archivos generados en \texttt{outputs/}
    \item Log de calidad en \verb|logs/data_quality.log|
\end{itemize}

\textbf{Fecha l√≠mite}: Viernes 23:59. \textbf{Puntaje extra}: Pipeline automatizado con funciones reutilizables.

\newpage
\section{Cap√≠tulo X: Visualizaci√≥n Profesional --- Dashboards Ejecutivos}

\subsection{Gr√°ficos de Control Estad√≠stico (SPC) --- Est√°ndar ISO 7870-2}

Los gr√°ficos SPC son \textbf{obligatorios} en sistemas de gesti√≥n de calidad (ISO 9001, ISO 22000). ISO 7870-2 establece una gu√≠a para el enfoque de gr√°ficos de control de Shewhart aplicado al control estad√≠stico de procesos.[web:60][web:61]

\begin{lstlisting}[language=Python, caption={Dashboard SPC profesional}, upquote=true]
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns

# Configuracion profesional
plt.style.use("seaborn-v0_8-whitegrid")
sns.set_palette("husl")

# Datos de control de pH (lotes 1-200)
df = pd.read_csv("data/raw/ph_lotes.csv", parse_dates=["fecha"], index_col="fecha")
df = df.sort_index()

# Calcular limites de control
media = df["ph"].mean()
std = df["ph"].std()
UCL = media + 3 * std
LCL = media - 3 * std
media_20 = df["ph"].rolling(20, center=True).mean()

# Grafico principal SPC
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=True)

# 1. Grafico de control principal
ax1.scatter(df.index, df["ph"], alpha=0.6, s=30, color="steelblue", label="Datos")
ax1.plot(df.index, media_20, color="orange", linewidth=2, label="Media movil 20 lotes")
ax1.axhline(y=media, color="green", linewidth=3, label=f"Media = {media:.2f}")
ax1.axhline(y=UCL, color="red", linestyle="--", linewidth=2, label=f"UCL = {UCL:.2f}")
ax1.axhline(y=LCL, color="red", linestyle="--", linewidth=2, label=f"LCL = {LCL:.2f}")
ax1.fill_between(df.index, LCL, UCL, alpha=0.1, color="red")
ax1.set_ylabel("pH", fontsize=12)
ax1.legend(loc="upper right")
ax1.set_title("Control Estadistico de Procesos - pH Lotes Cafe\nISO 7870-2",
              fontsize=14, fontweight="bold")
ax1.grid(True, alpha=0.3)

# 2. Senales especiales (reglas de Western Electric)
outliers = df[(df["ph"] > UCL) | (df["ph"] < LCL)]
ax1.scatter(outliers.index, outliers["ph"], color="red", s=100, marker="X",
            label=f"Senales especiales (n={len(outliers)})", zorder=5)

# 3. Grafico de residuos (diagnostico)
residuos = df["ph"] - media
ax2.plot(df.index, residuos, color="purple", linewidth=1.5)
ax2.axhline(y=0, color="black", linestyle="-", alpha=0.5)
ax2.axhline(y=2 * std, color="orange", linestyle="--", alpha=0.7)
ax2.axhline(y=-2 * std, color="orange", linestyle="--", alpha=0.7)
ax2.set_ylabel("Residuos (pH - Media)", fontsize=12)
ax2.set_xlabel("Fecha", fontsize=12)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig("outputs/spc_ph_professional.png", dpi=300, bbox_inches="tight")
plt.show()

# Tabla resumen SPC
spc_summary = pd.DataFrame({
    "Metrica": ["Media", "Std", "UCL", "LCL", "Senales especiales", "% en control"],
    "Valor": [f"{media:.2f}", f"{std:.2f}", f"{UCL:.2f}", f"{LCL:.2f}",
              len(outliers), f"{(1 - len(outliers) / len(df)) * 100:.1f}%"]
})
print(spc_summary)
spc_summary.to_csv("outputs/spc_summary.csv", index=False)
\end{lstlisting}

\begin{table}[h]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Se√±al especial} & \textbf{C√≥digo de detecci√≥n} \\
\midrule
Punto fuera de l√≠mites & \texttt{|x| > 3$\sigma$} \\
2 de 3 puntos $>$ 2$\sigma$ & Conteo en ventana de 3 \\
4 de 5 puntos $>$ 1$\sigma$ & Conteo en ventana de 5 \\
8 puntos consecutivos del mismo lado & Secuencia \\
Tendencia de 6 puntos & Secuencia creciente/decreciente \\
\bottomrule
\end{tabular}
\caption{Ejemplos de reglas de Western Electric para detectar causas especiales}
\end{table}

\subsection{Dashboards ejecutivos multi-panel}

\begin{lstlisting}[language=Python, caption={Dashboard OEE de 4 paneles}, upquote=true]
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

plt.style.use("seaborn-v0_8-whitegrid")

df = pd.read_csv("data/raw/produccion_resumen.csv")

fig = plt.figure(figsize=(16, 12))

# Panel 1: OEE por linea (barras)
ax1 = plt.subplot(2, 2, 1)
sns.barplot(data=df, x="linea", y="oee", palette="viridis", ax=ax1)
ax1.set_title("OEE por Linea de Produccion", fontweight="bold")
ax1.set_ylabel("OEE (%)")

# Panel 2: Boxplot rendimiento por turno
ax2 = plt.subplot(2, 2, 2)
sns.boxplot(data=df, x="turno", y="rendimiento_kgh", ax=ax2)
ax2.set_title("Distribucion de rendimiento por turno")
ax2.set_ylabel("kg/hora")

# Panel 3: Heatmap rechazos (linea x turno)
pivot_rechazos = df.pivot_table(values="rechazos", index="linea", columns="turno", aggfunc="sum")
ax3 = plt.subplot(2, 2, 3)
sns.heatmap(pivot_rechazos, annot=True, cmap="Reds", ax=ax3, cbar_kws={"label": "Rechazos"})
ax3.set_title("Heatmap de rechazos: linea x turno")

# Panel 4: Evolucion temporal OEE
ax4 = plt.subplot(2, 2, 4)
sns.lineplot(data=df, x="fecha", y="oee", hue="linea", marker="o", ax=ax4)
ax4.set_title("Evolucion de OEE por linea")
ax4.tick_params(axis="x", rotation=45)

plt.suptitle("Dashboard Ejecutivo - Planta Cafe Enero 2026", fontsize=16, fontweight="bold")
plt.tight_layout()
plt.savefig("outputs/dashboard_oee.png", dpi=300, bbox_inches="tight")
plt.show()
\end{lstlisting}

\begin{conceptbox}[Est√°ndares de visualizaci√≥n profesional]
\begin{itemize}
    \item \textbf{Colores institucionales}: Usar la paleta de la empresa.
    \item \textbf{T√≠tulos descriptivos}: No usar solo ``Gr√°fico 1''.
    \item \textbf{Unidades claras}: Incluir unidades en ejes y leyendas.
    \item \textbf{DPI 300}: Recomendado para impresi√≥n ejecutiva.
    \item \textbf{Formato PNG/PDF}: Evitar JPG por p√©rdida de calidad.
\end{itemize}
\end{conceptbox}

\begin{warningbox}[Errores comunes en dashboards]
\begin{itemize}
    \item Ejes truncados (sin 0).
    \item Colores sin significado (rojo no siempre significa ``malo'').
    \item Demasiados datos (por ejemplo, m√°s de 1000 puntos por gr√°fico).
    \item Falta de grilla o contextualizaci√≥n.
\end{itemize}
\end{warningbox}

\newpage
\section{Cap√≠tulo XI: Estad√≠stica Industrial con SciPy --- Toma de Decisiones Basada en Datos}

\subsection{Pruebas estad√≠sticas para ingenier√≠a de procesos}

En ingenier√≠a industrial, las pruebas de hip√≥tesis validan si las diferencias observadas son \textbf{estad√≠sticamente significativas} o solo ruido aleatorio. Esto es crucial para justificar inversiones en mejora de procesos.

\begin{lstlisting}[language=Python, caption={Paquete completo de pruebas industriales}, upquote=true]
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

# Dataset: Defectos por turno en 3 l√≠neas de producci√≥n
df = pd.read_csv("data/raw/defectos_por_turno.csv")

print("=== ANALISIS ESTADISTICO COMPLETO ===\n")

# 1. T-TEST: ¬øDiferencia significativa entre turnos?
print("1. T-TEST: Defectos Manana vs Noche")
manana = df[df["turno"] == "Ma√±ana"]["defectos_por_1000"]
noche = df[df["turno"] == "Noche"]["defectos_por_1000"]

t_stat, p_value = stats.ttest_ind(manana, noche, equal_var=False)  # Welch t-test
print(f"  t = {t_stat:.3f}, p = {p_value:.4f}")
print(f"  {'Rechazar H0' if p_value < 0.05 else 'No hay diferencia significativa'}")
print(f"  Conclusion: {'' if p_value >= 0.05 else 'El turno noche tiene mas defectos'}")

# 2. ANOVA: ¬øDiferencias entre las 3 l√≠neas?
print("\n2. ANOVA: Diferencias entre lineas L1, L2, L3")
l1 = df[df["linea"] == "L1"]["defectos_por_1000"]
l2 = df[df["linea"] == "L2"]["defectos_por_1000"]
l3 = df[df["linea"] == "L3"]["defectos_por_1000"]

f_stat, p_anova = stats.f_oneway(l1, l2, l3)
print(f"  F = {f_stat:.3f}, p = {p_anova:.4f}")
if p_anova < 0.05:
    # Post-hoc Tukey (si ANOVA rechaza H0)
    from statsmodels.stats.multicomp import pairwise_tukeyhsd
    tukey = pairwise_tukeyhsd(df["defectos_por_1000"], df["linea"], alpha=0.05)
    print("  Tukey HSD - Diferencias significativas:")
    print(tukey)

# 3. Prueba de normalidad (Shapiro-Wilk)
# Nota: para N > 5000, SciPy advierte que el p-value puede no ser preciso.
print("\n3. NORMALIDAD: ¬øDatos normales para usar t-test?")
muestra = df["defectos_por_1000"].dropna().sample(n=min(5000, df["defectos_por_1000"].dropna().shape[0]),
                                                 random_state=42)
stat_shapiro, p_shapiro = stats.shapiro(muestra)
print(f"  Shapiro-Wilk: W = {stat_shapiro:.3f}, p = {p_shapiro:.4f}")
print(f"  {'Normal' if p_shapiro > 0.05 else 'No normal (usar test no parametrico)'}")

# 4. Prueba no parametrica (si datos no normales): Mann-Whitney U
if p_shapiro < 0.05:
    stat_u, p_u = stats.mannwhitneyu(manana, noche, alternative="two-sided")
    print(f"\n4. Mann-Whitney U (no parametrico): U = {stat_u:.1f}, p = {p_u:.4f}")

# 5. Correlacion y prueba de significancia
print("\n5. CORRELACION: Duracion vs Defectos")
corr_coef, p_corr = stats.pearsonr(df["duracion_min"], df["defectos_por_1000"])
print(f"  Pearson r = {corr_coef:.3f}, p = {p_corr:.4f}")
print(f"  {'Correlacion significativa' if p_corr < 0.05 else 'Sin correlacion'}")
\end{lstlisting}

\begin{table}[h]
\centering
\begin{tabular}{@{}lllp{4cm}@{}}
\toprule
\textbf{Prueba} & \textbf{H0} & \textbf{p < 0.05} & \textbf{Decisi√≥n industrial} \\
\midrule
t-test & $\mu_1 = \mu_2$ & Rechazar & Cambiar proceso \\
ANOVA & $\mu_1 = \mu_2 = \mu_3$ & Rechazar & Investigar diferencias \\
Shapiro & Distribuci√≥n normal & Rechazar & Usar test no param√©trico \\
Pearson & $\rho = 0$ & Rechazar & Correlaci√≥n causal posible \\
\bottomrule
\end{tabular}
\caption{Interpretaci√≥n industrial de pruebas de hip√≥tesis}
\end{table}

\subsection{Tests de control de calidad espec√≠ficos}

\begin{lstlisting}[language=Python, caption={Tests para ingenier√≠a de procesos}, upquote=true]
# 6. Prueba de homogeneidad de varianzas (Levene)
levene_stat, levene_p = stats.levene(l1, l2, l3)
print(f"\n6. LEVENE (homogeneidad de varianzas): p = {levene_p:.4f}")
print(f"  {'Varianzas iguales' if levene_p > 0.05 else 'Varianzas diferentes (Welch ANOVA)'}")

# 7. Prueba Chi-cuadrado: Asociacion categorica (linea vs rechazo)
tabla_contingencia = pd.crosstab(df["linea"], df["rechazado"])
chi2, p_chi2, dof, expected = stats.chi2_contingency(tabla_contingencia)
print(f"\n7. CHI2 (linea vs rechazo): chi2 = {chi2:.2f}, p = {p_chi2:.4f}")
print("  Tabla esperada vs observada:")
print(pd.DataFrame(expected, index=tabla_contingencia.index,
                   columns=tabla_contingencia.columns).round(1))

# 8. CpK (Capability Index) - Capacidad de proceso
def calcular_cpk(data, limite_inf, limite_sup):
    media = data.mean()
    std = data.std()
    cpk_inf = (media - limite_inf) / (3 * std)
    cpk_sup = (limite_sup - media) / (3 * std)
    return min(cpk_inf, cpk_sup)

# Especificaciones pH: [6.2, 6.8]
cpk_ph = calcular_cpk(df["ph"].dropna(), 6.2, 6.8)
print(f"\n8. CpK pH [6.2-6.8]: {cpk_ph:.3f}")
print("  Interpretacion: " +
      ("Excelente (>1.67)" if cpk_ph > 1.67 else
       "Bueno (1.33-1.67)" if cpk_ph > 1.33 else
       "Marginal (1.00-1.33)" if cpk_ph > 1.0 else "Fuera de especificacion"))
\end{lstlisting}

\begin{industrybox}{Interpretaci√≥n para gerencia}
\begin{itemize}
    \item \textbf{p < 0.05} = Acci√≥n requerida (mejora de proceso)
    \item \textbf{CpK > 1.33} = Proceso capaz (Six Sigma)
    \item \textbf{Correlaci√≥n |r| > 0.7} = Variable candidata para optimizaci√≥n
\end{itemize}

Siempre comunicar: tama√±o muestral, supuestos de la prueba e interpretaci√≥n pr√°ctica.
\end{industrybox}

\begin{conceptbox}[Workflow estad√≠stico industrial]
\begin{enumerate}
    \item \textbf{Exploraci√≥n visual} (histogramas, boxplots)
    \item \textbf{Prueba de normalidad} (Shapiro-Wilk)
    \item \textbf{Test apropiado} (param√©trico/no param√©trico)
    \item \textbf{Post-hoc} si es necesario (Tukey)
    \item \textbf{Reporte ejecutivo} con p-value + interpretaci√≥n pr√°ctica
\end{enumerate}
\end{conceptbox}

\newpage

\section{Cap√≠tulo XII: SQL para Ciencia de Datos}

\subsection{Conexi√≥n y consultas}

\begin{lstlisting}[language=Python, caption={Pandas + SQL}, upquote=true]
import pandas as pd
from sqlalchemy import create_engine

engine = create_engine("postgresql://user:pass@localhost/planta")

df = pd.read_sql_query(
    "SELECT linea, AVG(kg) FROM lotes GROUP BY linea",
    engine
)
\end{lstlisting}

\section*{Referencias y recursos}

\subsection*{Bibliograf√≠a cient√≠fica}

\begin{enumerate}
    \item McKinney, W. (2017). \textit{Python for Data Analysis}. 2nd Edition. O'Reilly Media.
    \item VanderPlas, J. (2016). \textit{Python Data Science Handbook}. O'Reilly Media.
    \item Pandas Development Team (2024). \textit{Pandas Documentation}. \url{https://pandas.pydata.org/docs/}
\end{enumerate}

\subsection*{Est√°ndares industriales}

\begin{itemize}
    \item ISO 22000:2018 -- Food Safety Management Systems
    \item FDA 21 CFR Part 11 -- Electronic Records and Signatures
    \item Codex Alimentarius -- HACCP Principles
\end{itemize}

\subsection*{Datasets de pr√°ctica}

\begin{itemize}
    \item Kaggle: Food Safety Inspections
    \item UCI Machine Learning Repository: Wine Quality Dataset
    \item Open Food Facts: Global food products database
\end{itemize}

\end{document}
